{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Pegasus.client._client:Plan: [main] WARN  schema.JsonMetaSchema  - Unknown keyword $defs - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword examples - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "2020.06.01 20:35:37.756 UTC:    \n",
      "2020.06.01 20:35:37.762 UTC:   ----------------------------------------------------------------------- \n",
      "2020.06.01 20:35:37.768 UTC:   File for submitting this DAG to HTCondor           : 5.0.0dev-tutorial-wf-0.dag.condor.sub \n",
      "2020.06.01 20:35:37.774 UTC:   Log of DAGMan debugging messages                 : 5.0.0dev-tutorial-wf-0.dag.dagman.out \n",
      "2020.06.01 20:35:37.781 UTC:   Log of HTCondor library output                     : 5.0.0dev-tutorial-wf-0.dag.lib.out \n",
      "2020.06.01 20:35:37.788 UTC:   Log of HTCondor library error messages             : 5.0.0dev-tutorial-wf-0.dag.lib.err \n",
      "2020.06.01 20:35:37.796 UTC:   Log of the life of condor_dagman itself          : 5.0.0dev-tutorial-wf-0.dag.dagman.log \n",
      "2020.06.01 20:35:37.802 UTC:    \n",
      "2020.06.01 20:35:37.809 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: \n",
      "2020.06.01 20:35:37.822 UTC:   ----------------------------------------------------------------------- \n",
      "2020.06.01 20:35:38.811 UTC:   Created Pegasus database in: sqlite:////home/scitech/.pegasus/workflow.db \n",
      "2020.06.01 20:35:38.818 UTC:   Your database is compatible with Pegasus version: 5.0.0dev \n",
      "2020.06.01 20:35:39.875 UTC:   Created Pegasus database in: sqlite:////home/scitech/shared-data/sample-docker-wf/work/5.0.0dev-tutorial-wf-1591043735/5.0.0dev-tutorial-wf-0.replicas.db \n",
      "2020.06.01 20:35:39.889 UTC:   Your database is compatible with Pegasus version: 5.0.0dev \n",
      "2020.06.01 20:35:39.919 UTC:   Output replica catalog set to jdbc:sqlite:/home/scitech/shared-data/sample-docker-wf/work/5.0.0dev-tutorial-wf-1591043735/5.0.0dev-tutorial-wf-0.replicas.db \n",
      "2020.06.01 20:35:40.119 UTC:    \n",
      "2020.06.01 20:35:40.125 UTC:   Your workflow has been started and is running in the base directory: \n",
      "2020.06.01 20:35:40.131 UTC:    \n",
      "2020.06.01 20:35:40.138 UTC:   /home/scitech/shared-data/sample-docker-wf/work/5.0.0dev-tutorial-wf-1591043735 \n",
      "2020.06.01 20:35:40.144 UTC:    \n",
      "2020.06.01 20:35:40.150 UTC:   *** To monitor the workflow you can run *** \n",
      "2020.06.01 20:35:40.155 UTC:    \n",
      "2020.06.01 20:35:40.161 UTC:   pegasus-status -l /home/scitech/shared-data/sample-docker-wf/work/5.0.0dev-tutorial-wf-1591043735 \n",
      "2020.06.01 20:35:40.168 UTC:    \n",
      "2020.06.01 20:35:40.174 UTC:   *** To remove your workflow run *** \n",
      "2020.06.01 20:35:40.181 UTC:    \n",
      "2020.06.01 20:35:40.187 UTC:   pegasus-remove /home/scitech/shared-data/sample-docker-wf/work/5.0.0dev-tutorial-wf-1591043735 \n",
      "2020.06.01 20:35:40.324 UTC:   Time taken to execute is 4.311 seconds \n",
      " \n",
      " 2020.06.01 20:35:40.092 UTC: [ERROR]  Submitting to condor 5.0.0dev-tutorial-wf-0.dag.condor.sub \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;32m##################################################\u001b[0m] 100.0% ..Success (\u001b[1;32mCompleted: 12\u001b[0m, \u001b[1;33mQueued: 0\u001b[0m, \u001b[1;36mRunning: 0\u001b[0m, \u001b[1;31mFailed: 0\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# --- Import Pegasus API -------------------------------------------------------\n",
    "from Pegasus.api import *\n",
    "\n",
    "# --- Work Directory Setup -----------------------------------------------------\n",
    "RUN_ID = \"5.0.0dev-tutorial-wf-\" + datetime.now().strftime(\"%s\")\n",
    "TOP_DIR = Path.cwd()\n",
    "WORK_DIR = TOP_DIR / \"work\"\n",
    "\n",
    "try:\n",
    "    Path.mkdir(WORK_DIR)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# --- Configuration (Pegasus Properties) ---------------------------------------\n",
    "props = Properties()\n",
    "\n",
    "props[\"pegasus.data.configuration\"] = \"condorio\"\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"                                                                    \n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "\n",
    "# pegasus-planner will, by default, pick up this file in cwd\n",
    "props.write()\n",
    "\n",
    "# --- Site Catalog -------------------------------------------------------------\n",
    "# The Site Catalog describes the compute resources (which are often clusters) \n",
    "# that we intend to run the workflow upon. The SiteCatalog object and its usage\n",
    "# resembles that of the pre 5.0.0dev XML site catalogs. If you have an existing\n",
    "# XML site catalog, skip this step, and use pegasus-sc-converter to convert your\n",
    "# catalog from XML to YAML.\n",
    "\n",
    "\n",
    "sc = SiteCatalog()\n",
    "\n",
    "shared_scratch_dir = str(WORK_DIR / RUN_ID)\n",
    "local_storage_dir = str(WORK_DIR / \"outputs\" / RUN_ID)\n",
    "\n",
    "local = Site(\"local\")\\\n",
    "                .add_directories(\n",
    "                    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "                        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "                    \n",
    "                    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "                        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL))\n",
    "                )\n",
    "\n",
    "condorpool = Site(\"condorpool\")\\\n",
    "                .add_pegasus_profile(style=\"condor\")\\\n",
    "                .add_pegasus_profile(auxillary_local=\"true\")\\\n",
    "                .add_condor_profile(universe=\"vanilla\")\n",
    "\n",
    "sc.add_sites(local, condorpool)\n",
    "\n",
    "# pegasus-planner will, by default, pick up this file in cwd\n",
    "sc.write()\n",
    "\n",
    "# --- Transformation Catalog (Executables and Containers) ----------------------\n",
    "# In Pegasus lingo a transformation is the name for an executable, be it a \n",
    "# shell script, python script, or some compiled C code. We will catalog information\n",
    "# about these transformations as well as the containers they use in the \n",
    "# TransformationCatalog object. \n",
    "tc = TransformationCatalog()\n",
    "\n",
    "# Create and add our containers to the TransformationCatalog.\n",
    "\n",
    "# A container that will be used to execute the following two transformations.\n",
    "tools_container = Container(\n",
    "                    \"tools-container\", \n",
    "                    Container.DOCKER, \n",
    "                    image=\"docker:///ryantanaka/preprocess:latest\"\n",
    "                )\n",
    "\n",
    "tc.add_containers(tools_container)\n",
    "\n",
    "# Create and add our transformations to the TransformationCatalog.\n",
    "\n",
    "# An executable that is installed inside of \"tools_container\".\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/local/bin/preprocess.sh\",\n",
    "                is_stageable=False,\n",
    "                container=tools_container\n",
    "            )\n",
    "\n",
    "# A shell script that can be staged to some site and executed.\n",
    "process_text = Transformation(\n",
    "                    \"process_text.sh\", \n",
    "                    site=\"local\", \n",
    "                    pfn=str(Path(\".\").resolve() / \"bin/process_text.sh\"), \n",
    "                    is_stageable=True\n",
    "                )\n",
    "\n",
    "# A stageable python script that must be executed inside tools_container because\n",
    "# it contains packages that we have when we develop locally, but may not be \n",
    "# installed on a compute node. \n",
    "process_text_2nd_pass = Transformation(\n",
    "                            \"process_text_2nd_pass.py\",\n",
    "                            site=\"workflow-cloud\",\n",
    "                            pfn=\"http://www.isi.edu/~tanaka/process_text_2nd_pass.py\",\n",
    "                            is_stageable=True,\n",
    "                            container=tools_container\n",
    "                        )\n",
    "\n",
    "# An binary that is already installed on the condorpool site.\n",
    "tar = Transformation(\n",
    "        \"tar\",\n",
    "        site=\"condorpool\",\n",
    "        pfn=\"/usr/bin/tar\",\n",
    "        is_stageable=False\n",
    "    )\n",
    "\n",
    "tc.add_transformations(preprocess, process_text, process_text_2nd_pass, tar)\n",
    "\n",
    "# pegasus-planner will, by default, pick up this file in cwd\n",
    "tc.write()\n",
    "\n",
    "# --- Replica Catalog ----------------------------------------------------------\n",
    "# Any initial input files must be specified in the ReplicaCatalog object. In this\n",
    "# workflow, we have 1 input file to the workflow, and pegasus needs to know where\n",
    "# this file is located. We specify that when calling add_replica().\n",
    "\n",
    "initial_input_file = File(\"initial_input_file.txt\").add_metadata(size=54)\n",
    "\n",
    "rc = ReplicaCatalog()\\\n",
    "        .add_replica(\"local\", \"initial_input_file.txt\", str(Path(\".\").resolve() / \"initial_input_file.txt\"))\\\n",
    "        .write()\n",
    "\n",
    "# Again, pegasus-planner will know to look for this file in cwd.\n",
    "\n",
    "# --- Workflow -----------------------------------------------------------------\n",
    "# Set infer_dependencies=True so that they are inferred based on job\n",
    "# input and output file usage.\n",
    "wf = Workflow(\"5.0.0dev-tutorial-wf\", infer_dependencies=True)\n",
    "\n",
    "# Create Jobs. These objects store just that. The transformation (executable)\n",
    "# used by the job. The arguments passed to the executable. The input files used\n",
    "# and the output files produced. \n",
    "preprocessed_data = File(\"preprocessed_data.txt\")\n",
    "\n",
    "job_preprocess = Job(preprocess)\\\n",
    "                    .add_args(initial_input_file, preprocessed_data)\\\n",
    "                    .add_inputs(initial_input_file)\\\n",
    "                    .add_outputs(preprocessed_data)\n",
    "\n",
    "# Note that when passing File objects into add_args(), the file name is used.\n",
    "# For example, add_args(initial_input_file, preprocessed_data) above is \n",
    "# the same as add_args(initial_input_file, \"preprocessed_data.txt\"). \n",
    "\n",
    "processed_data = File(\"processed_data.txt\")\n",
    "\n",
    "job_process_text = Job(process_text)\\\n",
    "                    .add_args(preprocessed_data, processed_data)\\\n",
    "                    .add_inputs(preprocessed_data)\\\n",
    "                    .add_outputs(processed_data)\n",
    "\n",
    "twice_processed_data = File(\"twice_processed_data.txt\")\n",
    "extra_copy = File(\"backup.txt\")\n",
    "\n",
    "job_process_text_more = Job(process_text_2nd_pass)\\\n",
    "                            .add_args(processed_data, twice_processed_data, extra_copy)\\\n",
    "                            .add_inputs(processed_data)\\\n",
    "                            .add_outputs(twice_processed_data, extra_copy)\n",
    "\n",
    "result = File(\"scientific_results.tar.gz\")\n",
    "compress = Job(tar, _id=\"tar_job\")\\\n",
    "            .add_args(\"-cvzf\", result, twice_processed_data, extra_copy)\\\n",
    "            .add_inputs(*job_process_text_more.get_outputs())\\\n",
    "            .add_outputs(result)\n",
    "\n",
    "# With these jobs created, we can add them all to the workflow. The workflow\n",
    "# object will automatically assign ids to the jobs (if none is given) and then\n",
    "# determine the dependencies between them. \n",
    "wf.add_jobs(\n",
    "    job_preprocess,\n",
    "    job_process_text,\n",
    "    job_process_text_more,\n",
    "    compress\n",
    ")\n",
    "\n",
    "# Up until this point we have done the following:\n",
    "# 1. Created a pegasus properties file. (Pegasus settings)\n",
    "# 2. Created a site catalog. (Where will our workflow be executed?)\n",
    "# 3. Created a transformation catalog. (What executables will our workflow use?)\n",
    "# 4. Created a replica catalog. (Where are the initial input files used in this workflow located?)\n",
    "# 5. Created jobs, which use transformations, and make up our workflow. (How are these executables used?)\n",
    "\n",
    "# The next step is to run the workflow. This can be done through the wf object you\n",
    "# created above. If you have used pegasus-plan before, usage will be almost identical.\n",
    "try:\n",
    "    wf.plan(\n",
    "        dir=str(WORK_DIR),\n",
    "        relative_dir=RUN_ID,\n",
    "        submit=True\n",
    "    ).wait()\n",
    "except Exception as e:\n",
    "    print(e.args[1].stdout)\n",
    "    print(e.args[1].stderr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
